{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICTDS4 Gezamenlijke notebook\n",
    "\n",
    "In deze notebook staat alle code dat gemaakt is. \n",
    "\n",
    "De code dat hierinstaat is de samenvatting per type, om een beter beeld te krijgen naar de waarom's en hoe we het gedaan hebben refereren we naar de aparte jupyter notebooks in de models folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle imports nodig om de code te laten draaien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import platform\n",
    "import re\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from google.api_core import retry\n",
    "from keras import layers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Group: Data Processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Group: Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Group: Machine Learning\n",
    "import sklearn as sk\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Group: Deep Learning\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "# Group: Database\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "\n",
    "# Group: Miscellaneous\n",
    "import datetime\n",
    "import joblib\n",
    "import openai\n",
    "\n",
    "# Group: Google APIs\n",
    "import google.generativeai as palm\n",
    "from google.api_core import retry\n",
    "\n",
    "# Group: Utility\n",
    "import sys\n",
    "import platform\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the date of yesterday\n",
    "yesterday = datetime.date.today() - datetime.timedelta(days=1)\n",
    "yesterday_str = yesterday.strftime('%Y-%m-%d')\n",
    "\n",
    "# create the URL with the date of yesterday\n",
    "url = f'https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/?date_received_max={yesterday_str}&date_received_min=2011-12-01&field=all&format=csv&lens=product&no_aggs=true&product=Mortgage&size=375533&sub_lens=sub_product&trend_depth=5&trend_interval=month'\n",
    "\n",
    "parse_dates = ['Date received', 'Date sent to company']\n",
    "dtypes = {'Date received': str,\n",
    "          'Product': \"category\",\n",
    "          'Sub-product': \"category\",\n",
    "          'Issue': \"category\",\n",
    "          'Sub-issue': \"category\",\n",
    "          'Consumer complaint narrative': str,\n",
    "          'Company public response': str,\n",
    "          'Company': \"category\",\n",
    "          'State': \"category\",\n",
    "          'ZIP code': str,\n",
    "          'Tags': \"category\",\n",
    "          'Consumer consent provided?': str,\n",
    "          'Submitted via': \"category\",\n",
    "          'Date sent to company': str,\n",
    "          'Company response to consumer': str,\n",
    "          'Timely response?': str,\n",
    "          'Consumer disputed?': str,\n",
    "          'Complaint ID': int}\n",
    "\n",
    "DS1_data = pd.read_csv(url, low_memory=False, dtype=dtypes, parse_dates=parse_dates)\n",
    "DS1_data[['Timely response?', 'Consumer disputed?']] = DS1_data[['Consumer disputed?', 'Timely response?']].replace(\n",
    "    {'Yes': True, 'No': False}).astype(bool)\n",
    "DS1_data['Consumer consent provided?'] = DS1_data['Consumer consent provided?'].replace(\n",
    "    {'Consent provided': True, '': False}).astype(bool)\n",
    "DS1_data = DS1_data[pd.notnull(DS1_data['Consumer complaint narrative'])]  # Drops the missing rows with no  complaint.\n",
    "DS1_data.drop(columns=['Sub-issue', \"Product\"])\n",
    "\n",
    "db = sqlalchemy.create_engine('sqlite:///StaterData.db')\n",
    "DS1_data.to_sql(\"mortgage complaints\", db, if_exists=\"replace\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "conn = sqlite3.connect('StaterData.db')\n",
    "query = \"SELECT * FROM 'mortgage complaints'\"\n",
    "dtypes = {\n",
    "    'Date received': str,\n",
    "    'Product': \"category\",\n",
    "    'Sub-product': \"category\",\n",
    "    'Issue': \"category\",\n",
    "    'Sub-issue':\"category\",\n",
    "    'Consumer complaint narrative':str,\n",
    "    'Company public response':str,\n",
    "    'Company':\"category\",\n",
    "    'State':\"category\",\n",
    "    'ZIP code':str,\n",
    "    'Tags':\"category\",\n",
    "    'Consumer consent provided?':str,\n",
    "    'Submitted via':\"category\",\n",
    "    'Date sent to company':str,\n",
    "    'Company response to consumer':str,\n",
    "    'Timely response?':str,\n",
    "    'Consumer disputed?':str,\n",
    "    'Complaint ID':int\n",
    "}\n",
    "\n",
    "# Remove stopwords and return the text\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return \" \".join([token for token in nltk.word_tokenize(text) if token.lower() not in stop_words])\n",
    "\n",
    "# Clean the text\n",
    "def clean_text(text):\n",
    "    # Remove numerical values\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove punctuation marks\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove links and URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Remove leading/trailing white space and convert to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Lemmitize the word based on its part of speech (POS) tag\n",
    "def lemmatize_word(word, tag, lemmatizer):\n",
    "    wn_tag = None\n",
    "\n",
    "    # Map POS tag to WordNet POS tag\n",
    "    if tag.startswith('J'):\n",
    "        # Adjective\n",
    "        wn_tag = 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        # Verb\n",
    "        wn_tag = 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        # Noun\n",
    "        wn_tag = 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        # Adverb\n",
    "        wn_tag = 'r'\n",
    "\n",
    "    # Lemmatize the word\n",
    "    if wn_tag:\n",
    "        lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "    else:\n",
    "        lemma = word\n",
    "\n",
    "    return lemma\n",
    "\n",
    "# Lemmatize the sentence that is already tokenized\n",
    "def lemmatize_sentence(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Part-of-speech (POS) tag each word\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmas = [lemmatize_word(word, tag, lemmatizer) for word, tag in pos_tags]\n",
    "\n",
    "    # Join the lemmas back into a sentence\n",
    "    lemmatized_sentence = ' '.join(lemmas)\n",
    "\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def remove_non_alphabetica_char_and_x(text):\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove non alphabetical characters\n",
    "    alphabetical_text = [re.sub('[^a-zA-Z]+', '', word) for word in tokenized_text]\n",
    "\n",
    "    # Remove x from the text with regex\n",
    "    alphabetical_text = [word for word in alphabetical_text if not re.match('^x+$', word)]\n",
    "    return ' '.join(alphabetical_text)\n",
    "\n",
    "def clean_complaint(complaint):\n",
    "    cleaned_complaint = remove_stopwords(complaint)\n",
    "    cleaned_complaint = clean_text(cleaned_complaint)\n",
    "    cleaned_complaint = lemmatize_sentence(cleaned_complaint)\n",
    "    cleaned_complaint = remove_non_alphabetica_char_and_x(cleaned_complaint)\n",
    "    return cleaned_complaint\n",
    "\n",
    "# Read data\n",
    "data = pd.read_sql_query(query, conn, dtype=dtypes)\n",
    "df = data.copy()\n",
    "\n",
    "# Clean consumer complaint\n",
    "df['Clean consumer complaint'] = df['Consumer complaint narrative'].apply(lambda x: clean_complaint(x))\n",
    "\n",
    "df = df[['Complaint ID', 'Issue', 'Consumer complaint narrative', 'Clean consumer complaint']]\n",
    "\n",
    "# Save data in csv\n",
    "df.to_csv('StaterData.csv', index=False)\n",
    "print(\"Data is saved in csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditionele methodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Read csv, StaterData\n",
    "\n",
    "df = pd.read_csv('StaterData.csv')\n",
    "\n",
    "# Define your features and target, with x as text_column which contains the complaint in the dataset. And target_column which contains the target variable\n",
    "X = df['Clean consumer complaint']  \n",
    "y = df['Issue']                        \n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectorize your text data using a bag-of-words approach\n",
    "vectorizer = TfidfVectorizer(stop_words='english',token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "class_names = df['Issue']\n",
    "\n",
    "# Gridsearch Define the parameter grid\n",
    "best_params = {\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 14,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1\n",
    "    }\n",
    "\n",
    "# Create a Decision tree model\n",
    "model = DecisionTreeClassifier(random_state=2, **best_params)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# function to classify a complaint\n",
    "def questionDecisionTree(text):\n",
    "    question_decisionTree = vectorizer.transform([text])\n",
    "    prediction = model.predict(question_decisionTree)\n",
    "    return prediction[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned dataset created from complaintCleaner.py\n",
    "data = pd.read_csv('StaterData.csv')\n",
    "\n",
    "# Define your features and target\n",
    "X = data['Clean consumer complaint']  # text_column is the name of the column in your dataset that contains the text data\n",
    "y = data['Issue']  # target_column is the name of the column in your dataset that contains the target variable\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Define params from GridSearch which performed the best\n",
    "param_grid = {\n",
    "    'n_estimators': 200\n",
    "}\n",
    "\n",
    "# Vectorize your text data using TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=2)\n",
    "\n",
    "# Set the params for the model\n",
    "rf_model.set_params(**param_grid)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the testing data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "def text_to_prediction_random_forest(text):\n",
    "    custom_text_bow = vectorizer.transform([text])\n",
    "    predicted_issue = rf_model.predict(custom_text_bow)\n",
    "\n",
    "    return predicted_issue\n",
    "\n",
    "# Define your features and target\n",
    "X = data['Consumer complaint narrative']  # text_column is the name of the column in your dataset that contains the text data\n",
    "y = data['Issue']  # target_column is the name of the column in your dataset that contains the target variable\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectorize your text data using a bag-of-words approach\n",
    "vectorizer = TfidfVectorizer(stop_words=['english'])\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=2)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and importances\n",
    "feature_importances = pd.DataFrame({'feature': vectorizer.get_feature_names(), 'importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "top_features = feature_importances.head(10)['feature'].values.tolist()\n",
    "\n",
    "# Define your features and target\n",
    "X = data['Consumer complaint narrative']  # text_column is the name of the column in your dataset that contains the text data\n",
    "y = data['Issue']  # target_column is the name of the column in your dataset that contains the target variable\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectorize your text data using a bag-of-words approach\n",
    "vectorizer = TfidfVectorizer(stop_words='english', vocabulary=top_features)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=2)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the testing data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1score = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# # Define the parameter grid to search over\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [10, 20, 30, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "# }\n",
    "#\n",
    "# # Create a Random Forest model\n",
    "# rf_model = RandomForestClassifier(random_state=2)\n",
    "#\n",
    "# # Create a GridSearchCV object\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=rf_model,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,  # number of folds for cross-validation\n",
    "#     scoring='accuracy',\n",
    "#     # n_jobs=-1,  # use all available CPU cores\n",
    "# )\n",
    "#\n",
    "# # Fit the grid search to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# \n",
    "# # Print the best hyperparameters and the corresponding F1 score\n",
    "# print('Best hyperparameters:', grid_search.best_params_)\n",
    "# print('Best accuracy score:', grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multionominal logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'StaterData.csv'\n",
    "dtypes = {\n",
    "    'Date received': str,\n",
    "    'Product': \"category\",\n",
    "    'Sub-product': \"category\",\n",
    "    'Issue': \"category\",\n",
    "    'Sub-issue':\"category\",\n",
    "    'Consumer complaint narrative':str,\n",
    "    'Company public response':str,\n",
    "    'Company':\"category\",\n",
    "    'State':\"category\",\n",
    "    'ZIP code':str,\n",
    "    'Tags':\"category\",\n",
    "    'Consumer consent provided?':str,\n",
    "    'Submitted via':\"category\",\n",
    "    'Date sent to company':str,\n",
    "    'Company response to consumer':str,\n",
    "    'Timely response?':str,\n",
    "    'Consumer disputed?':str,\n",
    "    'Complaint ID':int,\n",
    "    'Clean consumer complaint':str,\n",
    "}\n",
    "\n",
    "# Read data from csv\n",
    "data = pd.read_csv(csv_file, dtype=dtypes)\n",
    "df = data[['Clean consumer complaint', 'Issue']].copy()\n",
    "\n",
    "# Split the data into training and testing sets: x = complaint and y = issue\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Clean consumer complaint'], df['Issue'], test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectorize the data with tfidf\n",
    "vectorizer = TfidfVectorizer(stop_words='english', token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Create multinomial logistic regression\n",
    "logregression = LogisticRegression(C=1, multi_class='multinomial', n_jobs=4, random_state=2, solver='saga')\n",
    "\n",
    "# Fit and predict\n",
    "logregression.fit(X_train_tfidf, y_train)\n",
    "y_pred = logregression.predict(X_test_tfidf)\n",
    "\n",
    "# function to classify a complaint\n",
    "def ask_question(text):\n",
    "    question_tfidf = vectorizer.transform([text])\n",
    "    prediction = logregression.predict(question_tfidf)\n",
    "    return prediction[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"StaterData.csv\")\n",
    "\n",
    "# split data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data['Clean consumer complaint'], data['Issue'], test_size=0.1, random_state=2)\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r'\\b[a-zA-Z]+\\b',\n",
    "    analyzer=\"word\",\n",
    "    use_idf=True\n",
    ")\n",
    "\n",
    "# Vectorize data\n",
    "vectorizer.fit(train_data)\n",
    "\n",
    "# If needed, use grid search to find best parameters\n",
    "# clf = SVC(kernel='linear')\n",
    "\n",
    "# parameters = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'gamma': ['scale', 'auto'],\n",
    "#     'class_weight': ['balanced', None],\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(clf, parameters, cv=5)\n",
    "# grid_search.fit(train_tfidf_vectors, train_labels)\n",
    "\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Set model parameters\n",
    "clf = SVC(C=10, class_weight='balanced', gamma='scale', kernel='linear')\n",
    "\n",
    "# Fit model\n",
    "train_tfidf_vectors = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# Transform test data\n",
    "test_tfidf_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# Fit model\n",
    "clf.fit(train_tfidf_vectors, train_labels)\n",
    "\n",
    "# Predict\n",
    "pred_labels = clf.predict(test_tfidf_vectors)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(test_labels, pred_labels, normalize=True)\n",
    "count = data.shape[0] // 1000\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}% with({count}k samples)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "\n",
    "# Define the data types for each column\n",
    "dtypes = {\n",
    "    'Date received': str,\n",
    "    'Product': \"category\",\n",
    "    'Sub-product': \"category\",\n",
    "    'Issue': \"category\",\n",
    "    'Sub-issue': \"category\",\n",
    "    'Consumer complaint narrative': str,\n",
    "    'Company public response': str,\n",
    "    'Company': \"category\",\n",
    "    'State': \"category\",\n",
    "    'ZIP code': str,\n",
    "    'Tags': \"category\",\n",
    "    'Consumer consent provided?': str,\n",
    "    'Submitted via': \"category\",\n",
    "    'Date sent to company': str,\n",
    "    'Company response to consumer': str,\n",
    "    'Timely response?': str,\n",
    "    'Consumer disputed?': str,\n",
    "    'Complaint ID': int\n",
    "}\n",
    "\n",
    "# Define the columns to parse as dates\n",
    "parse_dates = ['Product', 'Date received', 'Date sent to company']\n",
    "\n",
    "# Read the CSV file with specified data types and parse dates\n",
    "DS1_data = pd.read_csv(\"Data/complaints-2023-04-25_05_07.csv\", low_memory=False, dtype=dtypes, parse_dates=parse_dates)\n",
    "\n",
    "# Convert 'Timely response?' and 'Consumer disputed?' columns to boolean values\n",
    "DS1_data[['Timely response?', 'Consumer disputed?']] = DS1_data[['Timely response?', 'Consumer disputed?']].replace({'Yes': True, 'No': False}).astype(bool)\n",
    "\n",
    "# Convert 'Consumer consent provided?' column to boolean values\n",
    "DS1_data['Consumer consent provided?'] = DS1_data['Consumer consent provided?'].replace({'Consent provided': True, '': False}).astype(bool)\n",
    "\n",
    "# Drop rows with missing complaint narratives\n",
    "DS1_data.dropna(subset=['Consumer complaint narrative'], inplace=True)\n",
    "\n",
    "# Drop the 'Sub-issue' column as it is not needed\n",
    "DS1_data.drop(columns=['Sub-issue'], inplace=True)\n",
    "\n",
    "# Replace alle X occurences with emty strings, to avoid it from being the most important word.\n",
    "DS1_data['Consumer complaint narrative'] = DS1_data['Consumer complaint narrative'].str.replace('X', '')\n",
    "\n",
    "# Calculate the normalized count of issue categories\n",
    "IssueCountNormalized = DS1_data['Issue'].value_counts(normalize=True)\n",
    "\n",
    "# Create a TfidfVectorizer with optimized settings\n",
    "vectorizer = TfidfVectorizer(stop_words='english',              # Exclude common English words\n",
    "                             token_pattern=r'\\b[a-zA-Z]+\\b',    # Consider only alphabetic tokens\n",
    "                             analyzer='word',                   # Analyze at the word level\n",
    "                             use_idf=True,                      # Apply inverse document frequency weighting\n",
    "                             smooth_idf=True,                   # Apply smoothing to idf weights\n",
    "                             strip_accents='ascii',\n",
    "                             min_df=2,\n",
    "                             norm='l2')\n",
    "\n",
    "# Fit and transform the vectorizer to get an score per word in an array returned\n",
    "Vectorized_Data = vectorizer.fit_transform(DS1_data['Consumer complaint narrative'])\n",
    "\n",
    "# Add the scores array back into the dataframe.\n",
    "DS1_data['TF-IDF scores'] = list(Vectorized_Data.toarray())\n",
    "\n",
    "# Defines the dictionary of english words\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\n",
    "# Gets the list of words from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get unique values from the \"Issue\" column\n",
    "unique_issues = DS1_data['Issue'].unique()\n",
    "\n",
    "# Concatenate the unique issues into a single string\n",
    "all_issues = ' '.join(unique_issues)\n",
    "\n",
    "# Split the concatenated string into individual words\n",
    "all_words = all_issues.split()\n",
    "\n",
    "# Get unique words\n",
    "Mortgage_Terms = set(all_words)\n",
    "\n",
    "# Set an empty top 3 words list.\n",
    "top_words = []\n",
    "\n",
    "\n",
    "for i in range(Vectorized_Data.shape[0]):\n",
    "    print(i)\n",
    "    # Get the array with scores from this row\n",
    "    row_scores = Vectorized_Data[i].toarray()[0]\n",
    "\n",
    "    # Generate a dictionary with each word and then the Scores\n",
    "    scores_dict = {name: score for name, score in zip(feature_names,row_scores)}\n",
    "\n",
    "    # Loop over all the words and adjust the score\n",
    "    for name in feature_names:\n",
    "        # If term is in categoryname, add to score\n",
    "        if name in Mortgage_Terms:\n",
    "            scores_dict[name] *= (1 + 0.02*len(name) + 0.2)\n",
    "        else:\n",
    "            scores_dict[name] *= (1 + 0.02*len(name))\n",
    "\n",
    "    # Create a new array of adjusted scores in the same order as the feature names\n",
    "    adjusted_scores = np.zeros(len(feature_names))\n",
    "    for i, term in enumerate(feature_names):\n",
    "        adjusted_scores[i] = scores_dict[term]\n",
    "\n",
    "    #Set the top 3 to a empty list.\n",
    "    Top3Words = []\n",
    "\n",
    "    # Get the index of the highest scoring word\n",
    "    max_index = adjusted_scores.argmax()\n",
    "\n",
    "    # Iterate until 3 top words found or list of words empty\n",
    "    while max_index != 0 and len(Top3Words) <= 2:\n",
    "        #Check if there are any vowels in the topword, if not select new word\n",
    "        while True:\n",
    "            vowels = {\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"}\n",
    "            # Get the corresponding word\n",
    "            top_word = feature_names[max_index]\n",
    "            if any(char in vowels for char in feature_names[max_index]) and top_word in english_vocab:\n",
    "                Top3Words.append(top_word)\n",
    "            break\n",
    "\n",
    "        #Sets the current score of this word to 0 to select the second most popular word\n",
    "        adjusted_scores[max_index] = 0\n",
    "        max_index = adjusted_scores.argmax()\n",
    "    # Voeg het bijbehorende woord toe aan de lijst van top_words\n",
    "    top_words.append(Top3Words)\n",
    "\n",
    "# Add the list of topwords to the dataframe\n",
    "DS1_data['top_word'] = top_words\n",
    "\n",
    "# Create a TfidfVectorizer with optimized settings\n",
    "vectorizer = TfidfVectorizer(stop_words='english',              # Exclude common English words\n",
    "                             token_pattern=r'\\b[a-zA-Z]+\\b',    # Consider only alphabetic tokens\n",
    "                             analyzer='word',                   # Analyze at the word level\n",
    "                             use_idf=True,                      # Apply inverse document frequency weighting\n",
    "                             smooth_idf=True,                   # Apply smoothing to idf weights\n",
    "                             strip_accents='ascii',\n",
    "                             min_df=2,\n",
    "                             norm='l2')\n",
    "\n",
    "# Fit and transform the vectorizer to get an score per word in an array returned\n",
    "Vectorized_Data = vectorizer.fit_transform(data['Consumer complaint narrative'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Defines the dictionary of english words\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\n",
    "# Calculate the normalized count of issue categories\n",
    "IssueCountNormalized = data['Issue'].value_counts(normalize=True)\n",
    "\n",
    "# Get unique values from the \"Issue\" column\n",
    "unique_issues = data['Issue'].unique()\n",
    "\n",
    "# Concatenate the unique issues into a single string\n",
    "all_issues = ' '.join(unique_issues)\n",
    "\n",
    "# Split the concatenated string into individual words\n",
    "all_words = all_issues.split()\n",
    "\n",
    "# Get unique words\n",
    "Mortgage_Terms = set(all_words)\n",
    "\n",
    "# Function creates an array of every word in text and gives it an score\n",
    "def tfidf_custom_scoring(input_text):\n",
    "    # Fit the data to the vectorizer\n",
    "    vectorizer.fit(data['Consumer complaint narrative'])\n",
    "\n",
    "    # Adds the input from the user to the fit using an transform\n",
    "    transformed_data = vectorizer.transform([input_text])\n",
    "\n",
    "    # Get all the feature name(words instead of numbers) corresponding to the array\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # calculate initial scores and store them in a dictionary\n",
    "    scores_dict = {name: score for name, score in zip(feature_names, transformed_data.toarray()[0])}\n",
    "\n",
    "    # adjust scores based on term length and update the dictionary\n",
    "    for name in feature_names:\n",
    "        if name in Mortgage_Terms:\n",
    "            scores_dict[name] *= (1 + 0.01*len(name) + 1.5)\n",
    "        else:\n",
    "            scores_dict[name] *= (1 + 0.01*len(name))\n",
    "\n",
    "    # create a new array of adjusted scores in the same order as the feature names\n",
    "    adjusted_scores = np.zeros(len(feature_names))\n",
    "    for INDEX, term in enumerate(feature_names):\n",
    "        adjusted_scores[INDEX] = scores_dict[term]\n",
    "\n",
    "    # return a tuple of the feature names and adjusted scores\n",
    "    return feature_names, adjusted_scores\n",
    "\n",
    "def classifiyComplaintTFIDF(user_input):\n",
    "    # Get an array of words with corresponding scores\n",
    "    data = tfidf_custom_scoring(user_input)\n",
    "\n",
    "    # Create a dictionary with feature names as keys and scores as values\n",
    "    scores_dict = {name: score for name, score in zip(data[0], data[1])}\n",
    "\n",
    "    # Create a new array of adjusted scores in the same order as the feature names\n",
    "    adjusted_scores = np.zeros(len(feature_names))\n",
    "    for i, term in enumerate(feature_names):\n",
    "        adjusted_scores[i] = scores_dict[term]\n",
    "\n",
    "    # Set the top 3 words to a list\n",
    "    Top3Words = []\n",
    "\n",
    "    def get_top3_words():\n",
    "        # Get the index of the highest score in the scores array\n",
    "        index_max = adjusted_scores.argmax()\n",
    "\n",
    "        while index_max != 0 and len(Top3Words) <= 2:\n",
    "            # Check if there are any vowels in the top word, if not, select a new word\n",
    "            while True:\n",
    "                vowels = {\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"}\n",
    "                # Get the corresponding word\n",
    "                top_word = feature_names[index_max]\n",
    "                if any(char in vowels for char in top_word) and top_word in english_vocab:\n",
    "                    Top3Words.append(top_word)\n",
    "                break\n",
    "\n",
    "            # Get the corresponding word\n",
    "            top_word = str(data[0][index_max])\n",
    "\n",
    "            # If the word is in the English vocabulary, add it to the top 3 list\n",
    "            if top_word in english_vocab:\n",
    "                Top3Words.append(top_word)\n",
    "\n",
    "            # Set the current score of this word to 0 to select the second most popular word\n",
    "            adjusted_scores[index_max] = 0\n",
    "            index_max = adjusted_scores.argmax()\n",
    "\n",
    "    get_top3_words()\n",
    "\n",
    "    # Check all past result classifications\n",
    "    def check_corresponding_word(relevant_word):\n",
    "        return data[(data[\"top_word\"].str[0] == relevant_word[0]) & data[\"top_word\"].str[1].isin(relevant_word)]\n",
    "\n",
    "    filtered_df = check_corresponding_word(Top3Words)\n",
    "\n",
    "    # Count the occurrences of each issue and get the most common one, as long as there are words left\n",
    "    value_counts = filtered_df[\"Issue\"].value_counts()\n",
    "    \n",
    "    # Concatenate the 'IssueCountNormalized' and 'value_counts' dataframes along the columns axis\n",
    "    NormalizedTable = pd.concat([IssueCountNormalized, value_counts], axis=1, keys=('perc', 'valuecount'))\n",
    "\n",
    "    # Calculate the 'Endscores' by dividing the 'valuecount' column by the 'perc' column in 'NormalizedTable'\n",
    "    Endscores = NormalizedTable.valuecount / NormalizedTable.perc\n",
    "\n",
    "    # Assign the calculated 'Endscores' as a new column in 'NormalizedTable'\n",
    "    NormalizedTable[\"Endscores\"] = Endscores\n",
    "\n",
    "    # Create a new column in 'NormalizedTable' containing the keys from 'IssueCountNormalized' converted to a list\n",
    "    NormalizedTable[\"IssueName\"] = IssueCountNormalized.keys().tolist()\n",
    "\n",
    "    # Locate the row in 'NormalizedTable' with the maximum value in the 'Endscores' column\n",
    "    Toprow = NormalizedTable.loc[NormalizedTable['Endscores'].idxmax()]\n",
    "\n",
    "    # Return the value in the 'IssueName' column of the row with the highest 'Endscores' value\n",
    "    return Toprow.IssueName\n",
    "\n",
    "print(\"Your question will be in the following category: \"+classifiyComplaintTFIDF(input(\"What question do you want to categorize? \")))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articial Intelligence / machine learning methodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT-2 (Random forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned dataset created from complaintCleaner.py\n",
    "data = pd.read_csv('StaterData.csv')\n",
    "\n",
    "# Define your features and target\n",
    "X = data['Clean consumer complaint']  # text_column is the name of the column in your dataset that contains the text data\n",
    "y = data['Issue']  # target_column is the name of the column in your dataset that contains the target variable\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Define params from GridSearch which performed the best\n",
    "param_grid = {\n",
    "    'n_estimators': 200\n",
    "}\n",
    "\n",
    "# Vectorize your text data using TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=2)\n",
    "\n",
    "# Set the params for the model\n",
    "rf_model.set_params(**param_grid)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the testing data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "def text_to_prediction_random_forest(text):\n",
    "    custom_text_bow = vectorizer.transform([text])\n",
    "    predicted_issue = rf_model.predict(custom_text_bow)\n",
    "\n",
    "    return predicted_issue\n",
    "\n",
    "# Define your features and target\n",
    "X = data['Consumer complaint narrative']  # text_column is the name of the column in your dataset that contains the text data\n",
    "y = data['Issue']  # target_column is the name of the column in your dataset that contains the target variable\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectorize your text data using a bag-of-words approach\n",
    "vectorizer = TfidfVectorizer(stop_words=['english'])\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=2)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and importances\n",
    "feature_importances = pd.DataFrame({'feature': vectorizer.get_feature_names(), 'importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "top_features = feature_importances.head(10)['feature'].values.tolist()\n",
    "\n",
    "# Define your features and target\n",
    "X = data['Consumer complaint narrative']  # text_column is the name of the column in your dataset that contains the text data\n",
    "y = data['Issue']  # target_column is the name of the column in your dataset that contains the target variable\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectorize your text data using a bag-of-words approach\n",
    "vectorizer = TfidfVectorizer(stop_words='english', vocabulary=top_features)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=2)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the testing data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1score = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# # Define the parameter grid to search over\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [10, 20, 30, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "# }\n",
    "#\n",
    "# # Create a Random Forest model\n",
    "# rf_model = RandomForestClassifier(random_state=2)\n",
    "#\n",
    "# # Create a GridSearchCV object\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=rf_model,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,  # number of folds for cross-validation\n",
    "#     scoring='accuracy',\n",
    "#     # n_jobs=-1,  # use all available CPU cores\n",
    "# )\n",
    "#\n",
    "# # Fit the grid search to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# \n",
    "# # Print the best hyperparameters and the corresponding F1 score\n",
    "# print('Best hyperparameters:', grid_search.best_params_)\n",
    "# print('Best accuracy score:', grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaLM 2 (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print kernal stats\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n",
    "\n",
    "# Select the right model\n",
    "palm.configure(api_key='<>')\n",
    "models = [m for m in palm.list_models() if 'embedText' in m.supported_generation_methods]\n",
    "model = models[0]\n",
    "\n",
    "# Import the data, and spit the data\n",
    "data = pd.read_csv(\"StaterData.csv\")\n",
    "df_train, df_test = train_test_split(data, test_size=0.3, random_state=2)\n",
    "\n",
    "# Python progress bar \n",
    "tqdm.pandas()\n",
    "\n",
    "def make_embed_text_fn(model):\n",
    "  @retry.Retry(timeout=300.0)\n",
    "  def embed_fn(text: str) -> list[float]:\n",
    "    # Using the palm model generate the embeddings for \n",
    "    return palm.generate_embeddings(model=model, text=text)['embedding']\n",
    "  return embed_fn\n",
    "\n",
    "# Creates the embedding of the send dataframe\n",
    "def create_embeddings(model, data):\n",
    "  # Adds the column embeddings with the corrosponding embedding\n",
    "  data['Embeddings'] = data['Clean consumer complaint'].progress_apply(make_embed_text_fn(model))\n",
    "  return data\n",
    "\n",
    "# Creates the embedding for the train and the testdata\n",
    "df_train = create_embeddings(model, df_train)\n",
    "df_test = create_embeddings(model, df_test)\n",
    "\n",
    "# Calling the API from Palm and convert the text to vectors\n",
    "def build_classification_model(input_size: int, num_classes: int) -> keras.Model:\n",
    "  inputs = x = keras.Input(input_size)\n",
    "  x = layers.Dense(input_size, activation='relu')(x)\n",
    "  x = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "  return keras.Model(inputs=[inputs], outputs=x)\n",
    "\n",
    "# Derive the embedding size from the first training element.\n",
    "embedding_size = len(df_train['Embeddings'].iloc[0])\n",
    "\n",
    "# Give your model a different name, as you have already used the variable name 'model'\n",
    "classifier = build_classification_model(embedding_size, len(df_train['Clean consumer complaint'].unique()))\n",
    "classifier.summary()\n",
    "\n",
    "classifier.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                   optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Tensorflow model training\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Split the x and y components of the train and validation subsets.\n",
    "y_train = df_train['Issue_Int']\n",
    "x_train = np.stack(df_train['Embeddings'])\n",
    "y_val = df_test['Issue_Int']\n",
    "x_val = np.stack(df_test['Embeddings'])\n",
    "\n",
    "# Train the model for the desired number of epochs.\n",
    "callback = keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
    "\n",
    "history = classifier.fit(x=x_train,\n",
    "                         y=y_train,\n",
    "                         validation_data=(x_val, y_val),\n",
    "                         callbacks=[callback],\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         epochs=NUM_EPOCHS,) \n",
    "\n",
    "classifier.evaluate(x=x_val, y=y_val, return_dict=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
