{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MakerSuite embeddings, using embeddings to convert the text to vectorized and on that classify it using SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "\n",
    "import re\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.api_core import retry\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that the kernals are correctly in use and that all resources are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the api key and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key='<>')\n",
    "models = [m for m in palm.list_models() if 'embedText' in m.supported_generation_methods]\n",
    "\n",
    "model = models[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data and create a train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data, and spit the data\n",
    "data = pd.read_csv(\"StaterData.csv\")\n",
    "\n",
    "issue_to_int = {\n",
    "    \"Trouble during payment process\": 0,\n",
    "    \"Struggling to pay mortgage\": 1,\n",
    "    \"Loan servicing, payments, escrow account\": 2,\n",
    "    \"Applying for a mortgage or refinancing an existing mortgage\": 3,\n",
    "    \"Loan modification,collection,foreclosure\": 4,\n",
    "    \"Closing on a mortgage\": 5,\n",
    "    \"Application, originator, mortgage broker\": 6,\n",
    "    \"Credit decision / Underwriting\": 7,\n",
    "    \"Incorrect information on your report\": 8,\n",
    "    \"Settlement process and costs\": 9,\n",
    "    \"Problem with a credit reporting company's investigation into an existing problem\": 10,\n",
    "    \"Improper use of your report\": 11,\n",
    "    \"Credit monitoring or identity theft protection services\": 12,\n",
    "}\n",
    "\n",
    "# Create a new column called `Issue_Int`\n",
    "data[\"Issue_Int\"] = data[\"Issue\"].map(issue_to_int)\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.3, random_state=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the API from Palm and convert the text to vectors, PaLM api limit is currently 4 items per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e754bb34584f32a8465a2a4b4371d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python progress bar \n",
    "tqdm.pandas()\n",
    "\n",
    "def make_embed_text_fn(model):\n",
    "  @retry.Retry(timeout=300.0)\n",
    "  def embed_fn(text: str) -> list[float]:\n",
    "    # Using the palm model generate the embeddings for \n",
    "    return palm.generate_embeddings(model=model, text=text)['embedding']\n",
    "  return embed_fn\n",
    "\n",
    "# Creates the embedding of the send dataframe\n",
    "def create_embeddings(model, data):\n",
    "  # Adds the column embeddings with the corrosponding embedding\n",
    "  data['Embeddings'] = data['Consumer complaint narrative'].progress_apply(make_embed_text_fn(model))\n",
    "  return data\n",
    "\n",
    "# Creates the embedding for the train and the testdata\n",
    "data = create_embeddings(model, data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Keras model, used for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classification_model(input_size: int, num_classes: int) -> keras.Model:\n",
    "  inputs = x = keras.Input(input_size)\n",
    "  x = layers.Dense(input_size, activation='relu')(x)\n",
    "  x = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "  return keras.Model(inputs=[inputs], outputs=x)\n",
    "\n",
    "# Derive the embedding size from the first training element.\n",
    "embedding_size = len(df_train['Embeddings'].iloc[0])\n",
    "\n",
    "# Give your model a different name, as you have already used the variable name 'model'\n",
    "classifier = build_classification_model(embedding_size, len(df_train['Consumer complaint narrative'].unique()))\n",
    "classifier.summary()\n",
    "\n",
    "classifier.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                   optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "                   metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Split the x and y components of the train and validation subsets.\n",
    "y_train = df_train['Issue_Int']\n",
    "x_train = np.stack(df_train['Embeddings'])\n",
    "y_val = df_test['Issue_Int']\n",
    "x_val = np.stack(df_test['Embeddings'])\n",
    "\n",
    "# Train the model for the desired number of epochs.\n",
    "callback = keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
    "\n",
    "history = classifier.fit(x=x_train,\n",
    "                         y=y_train,\n",
    "                         validation_data=(x_val, y_val),\n",
    "                         callbacks=[callback],\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         epochs=NUM_EPOCHS,) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate how accurate the model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(x=x_val, y=y_val, return_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 plots that show the accuracy over the amounts of epochs and loses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "  fig.set_size_inches(20, 8)\n",
    "\n",
    "  # Plot loss\n",
    "  ax1.set_title('Loss')\n",
    "  ax1.plot(history.history['loss'], label = 'train')\n",
    "  ax1.plot(history.history['val_loss'], label = 'test')\n",
    "  ax1.set_ylabel('Loss')\n",
    "\n",
    "  ax1.set_xlabel('Epoch')\n",
    "  ax1.legend(['Train', 'Validation'])\n",
    "\n",
    "  # Plot accuracy\n",
    "  ax2.set_title('Accuracy')\n",
    "  ax2.plot(history.history['accuracy'],  label = 'train')\n",
    "  ax2.plot(history.history['val_accuracy'], label = 'test')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_xlabel('Epoch')\n",
    "  ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows great promise, without cleaning the text the model almost gives the same accuracy as using TF-IDF with cleaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
