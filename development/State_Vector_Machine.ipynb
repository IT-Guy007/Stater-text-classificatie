{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Support Vector machine model, using TF-IDF to convert text to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T13:21:26.480857Z",
     "start_time": "2023-05-29T13:21:26.463326Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Exporting model\n",
    "import joblib\n",
    "\n",
    "# Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word normalizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import edit_distance\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download(\"words\")\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Cleaning unneeded repetitive words\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Import data from the data file, that is produced by the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql_query(\"SELECT * FROM 'mortgage complaints'\", \"sqlite:///StaterData.db\", parse_dates={'Date received': '%Y-%m-%d %H:%M:%S', 'Date sent to company': '%Y-%m-%d %H:%M:%S'})\n",
    "\n",
    "# Cleaning\n",
    "data[['Timely response?','Consumer disputed?']] = data[['Consumer disputed?','Timely response?']].replace({'Yes': True, 'No':False}).astype(bool)\n",
    "data['Consumer consent provided?'] = data['Consumer consent provided?'].replace({'Consent provided': True, '':False}).astype(bool)\n",
    "data[\"Consumer complaint narrative\"] = data[data['Consumer complaint narrative'].notna()]\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].str.lower()\n",
    "\n",
    "data = data.drop(columns=['Sub-issue'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Removing stopwords, by removing stopwords the algorithm will focus on the important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "remove_stopwords = lambda text: \" \".join([token for token in nltk.word_tokenize(text) if token.lower() not in stop_words])\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(remove_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Even though the dataset is anonymised the model cant train on names, because of ethics. It could be that if the model is trained on a different dataset with names that some people will get a disadvantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "def remove_names(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            text = text.replace(ent.text, '')\n",
    "    return text\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(remove_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Remove text that doesn't add context, like hashtags, urls, commas, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove numerical values\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation marks\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove links and URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove leading/trailing white space and convert to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(clean_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Normalising words, making every word the normal tense. That gives the algorithm a better opportunity to create patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = SpellChecker()\n",
    "\n",
    "def lemmatize_word(word, tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        # Adjective\n",
    "        wn_tag = 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        # Verb\n",
    "        wn_tag = 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        # Noun\n",
    "        wn_tag = 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        # Adverb\n",
    "        wn_tag = 'r'\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    \n",
    "    if wn_tag:\n",
    "        lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "    else:\n",
    "        lemma = word\n",
    "    \n",
    "    return lemma\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Part-of-speech (POS) tag each word\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Iterate over each word and perform spell correction\n",
    "    corrected_tokens = [spell.correction(word) for word, _ in pos_tags]\n",
    "    \n",
    "    # Lemmatize each corrected word based on its POS tag\n",
    "    lemmas = [lemmatize_word(word, tag) for word, tag in zip(corrected_tokens, pos_tags)]\n",
    "    \n",
    "    # Join the lemmas back into a sentence\n",
    "    lemmatized_sentence = ' '.join(lemmas)\n",
    "    \n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(lemmatize_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tokenization, set the words into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Consumer complaint narrative']= [word_tokenize(entry) for entry in data['Consumer complaint narrative']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Removing the anonymized data, that have been changed into x's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(lambda x: [re.sub('[^a-zA-Z]+', '', word) for word in x])\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(lambda x: [word for word in x if not re.match('^x+$', word)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(data['Consumer complaint narrative'], data['Issue'], test_size=0.1)\n",
    "\n",
    "train_data = [' '.join(tokens) for tokens in train_data]\n",
    "test_data = [' '.join(tokens) for tokens in test_data]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a TF-IDF vectorizer and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r'\\b[a-zA-Z]+\\b',\n",
    "    analyzer=\"word\",\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    norm=None,\n",
    "    tokenizer=None,\n",
    "    preprocessor=None\n",
    ")\n",
    "vectorizer.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the best settings, this code is only run once and gives the best options for the settings for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a support vector machine classifier.\n",
    "# clf = SVC(kernel='linear')\n",
    "\n",
    "# # Define the parameters to be tuned.\n",
    "# parameters = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'gamma': ['scale', 'auto'],\n",
    "#     'class_weight': ['balanced', None],\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV object to search over the parameter grid.\n",
    "# grid_search = GridSearchCV(clf, parameters, cv=5)\n",
    "\n",
    "# # Fit the GridSearchCV object to the training data.\n",
    "# grid_search.fit(train_tfidf_vectors, train_labels)\n",
    "\n",
    "# # Print the best hyperparameters and the corresponding mean cross-validated score.\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Creating SVM and fitting the vectorizors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a support vector machine classifier.\n",
    "clf = SVC(C=10, class_weight='balanced', gamma='scale', kernel='linear')\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "train_tfidf_vectors = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# Transform the testing data using the fitted vectorizer\n",
    "test_tfidf_vectors = vectorizer.transform(test_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train the classifier on the TF-IDF vectors. Takes the longest time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_tfidf_vectors, train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Predicting and exporting for long term usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the testing data\n",
    "pred_labels = clf.predict(test_tfidf_vectors)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(test_labels, pred_labels, normalize=True)\n",
    "count = data.shape[0] // 1000\n",
    "\n",
    "# Export the model to a file.\n",
    "filename = f\"model({count}K, {accuracy:.1%}).joblib\"\n",
    "joblib.dump(clf, filename)\n",
    "\n",
    "print(f\"Exported to: {filename}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create classification report for more insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test data.\n",
    "predictions = clf.predict(test_tfidf_vectors)\n",
    "\n",
    "# Create and print the classification report\n",
    "report = classification_report(test_labels, predictions, zero_division=1)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Test the model with a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new question\n",
    "new_question = \"I need the credit score, but the website doesnt load\"\n",
    "\n",
    "# Transform the new question using the TF-IDF vectorizer\n",
    "new_question_vector = vectorizer.transform([new_question])\n",
    "\n",
    "# Use the classifier to predict the label for the new question\n",
    "predicted_label = clf.predict(new_question_vector)\n",
    "\n",
    "# Print the predicted label\n",
    "print(predicted_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported to a file and gives an accuracy of 59%. This is close to the other models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
