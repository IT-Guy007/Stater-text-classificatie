{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing the libraries\n",
    "import pandas as pd\n",
    "import ssl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Exporting model\n",
    "import joblib\n",
    "\n",
    "# Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word normalizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Cleaning unneeded repetitive words\n",
    "import re\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1_data = pd.read_sql_query(\"SELECT * FROM 'mortgage complaints'\", \"sqlite:///StaterData.db\", parse_dates={'Date received': '%Y-%m-%d %H:%M:%S', 'Date sent to company': '%Y-%m-%d %H:%M:%S'})\n",
    "\n",
    "# Cleaning\n",
    "DS1_data[['Timely response?','Consumer disputed?']] = DS1_data[['Consumer disputed?','Timely response?']].replace({'Yes': True, 'No':False}).astype(bool)\n",
    "DS1_data['Consumer consent provided?'] = DS1_data['Consumer consent provided?'].replace({'Consent provided': True, '':False}).astype(bool)\n",
    "DS1_data[DS1_data['Consumer complaint narrative'].notna()]\n",
    "DS1_data['Consumer complaint narrative'] = DS1_data['Consumer complaint narrative'].str.lower()\n",
    "\n",
    "data = DS1_data.drop(columns=['Sub-issue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trouble during payment process                                                      33782\n",
       "Struggling to pay mortgage                                                          17439\n",
       "Loan servicing, payments, escrow account                                            14721\n",
       "Loan modification,collection,foreclosure                                            10789\n",
       "Applying for a mortgage or refinancing an existing mortgage                         10490\n",
       "Closing on a mortgage                                                                7408\n",
       "Application, originator, mortgage broker                                             3746\n",
       "Settlement process and costs                                                         2249\n",
       "Incorrect information on your report                                                 1420\n",
       "Credit decision / Underwriting                                                       1289\n",
       "Problem with a credit reporting company's investigation into an existing problem      622\n",
       "Improper use of your report                                                           194\n",
       "Credit monitoring or identity theft protection services                                49\n",
       "Unable to get your credit report or credit score                                       47\n",
       "Problem with fraud alerts or security freezes                                          19\n",
       "Name: Issue, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from it\n",
    "# data = DS1_data.sample(n=250)\n",
    "data[\"Issue\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "remove_stopwords = lambda text: \" \".join([token for token in nltk.word_tokenize(text) if token.lower() not in stop_words])\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise free text\n",
    "# Removing hashtags, urls, commas, etc.\n",
    "def clean_text(text):\n",
    "    # Remove numerical values\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation marks\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove links and URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove leading/trailing white space and convert to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing words, (play, playing, played) -> play\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to lemmatize a single word based on its part of speech (POS) tag\n",
    "def lemmatize_word(word, tag):\n",
    "    # Map POS tag to WordNet POS tag\n",
    "    if tag.startswith('J'):\n",
    "        # Adjective\n",
    "        wn_tag = 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        # Verb\n",
    "        wn_tag = 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        # Noun\n",
    "        wn_tag = 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        # Adverb\n",
    "        wn_tag = 'r'\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    \n",
    "    # Lemmatize the word\n",
    "    if wn_tag:\n",
    "        lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "    else:\n",
    "        lemma = word\n",
    "    \n",
    "    return lemma\n",
    "\n",
    "# Define a function to lemmatize a sentence\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Part-of-speech (POS) tag each word\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmas = [lemmatize_word(word, tag) for word, tag in pos_tags]\n",
    "    \n",
    "    # Join the lemmas back into a sentence\n",
    "    lemmatized_sentence = ' '.join(lemmas)\n",
    "    \n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, creating into sets of words\n",
    "data['Consumer complaint narrative']= [word_tokenize(entry) for entry in data['Consumer complaint narrative']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove only tokenized words that are not alphabetic or only x\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(lambda x: [re.sub('[^a-zA-Z]+', '', word) for word in x])\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(lambda x: [word for word in x if not re.match('^x+$', word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets.\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data['Consumer complaint narrative'], data['Issue'], test_size=0.1)\n",
    "\n",
    "train_data = [' '.join(tokens) for tokens in train_data]\n",
    "test_data = [' '.join(tokens) for tokens in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(norm=None, stop_words='english',\n",
       "                token_pattern='\\\\b[a-zA-Z]+\\\\b')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TF-IDF vectorizer.\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r'\\b[a-zA-Z]+\\b',\n",
    "    analyzer=\"word\",\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    norm=None,\n",
    "    tokenizer=None,\n",
    "    preprocessor=None\n",
    ")\n",
    "# Fit the vectorizer to the training data.\n",
    "vectorizer.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer object with unigrams, bigrams, and trigrams as features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# Create a support vector machine classifier.\n",
    "clf = SVC(C=10, class_weight='balanced', gamma='scale', kernel='linear')\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "train_tfidf_vectors = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# Transform the testing data using the fitted vectorizer\n",
    "test_tfidf_vectors = vectorizer.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a support vector machine classifier.\n",
    "# clf = SVC(kernel='linear')\n",
    "\n",
    "# # Define the parameters to be tuned.\n",
    "# parameters = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'gamma': ['scale', 'auto'],\n",
    "#     'class_weight': ['balanced', None],\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV object to search over the parameter grid.\n",
    "# grid_search = GridSearchCV(clf, parameters, cv=5)\n",
    "\n",
    "# # Fit the GridSearchCV object to the training data.\n",
    "# grid_search.fit(train_tfidf_vectors, train_labels)\n",
    "\n",
    "# # Print the best hyperparameters and the corresponding mean cross-validated score.\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, class_weight='balanced', kernel='linear')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the TF-IDF vectors.\n",
    "clf.fit(train_tfidf_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to: model(104K, 58.5%).joblib\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels of the testing data\n",
    "pred_labels = clf.predict(test_tfidf_vectors)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(test_labels, pred_labels, normalize=True)\n",
    "count = data.shape[0] // 1000\n",
    "\n",
    "# Export the model to a file.\n",
    "filename = f\"model({count}K, {accuracy:.1%}).joblib\"\n",
    "joblib.dump(clf, filename)\n",
    "\n",
    "print(f\"Exported to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                  precision    recall  f1-score   support\n",
      "\n",
      "                                        Application, originator, mortgage broker       0.37      0.14      0.21       366\n",
      "                     Applying for a mortgage or refinancing an existing mortgage       0.54      0.62      0.58      1054\n",
      "                                                           Closing on a mortgage       0.48      0.47      0.48       742\n",
      "                                                  Credit decision / Underwriting       0.33      0.01      0.01       140\n",
      "                         Credit monitoring or identity theft protection services       1.00      0.25      0.40         4\n",
      "                                                     Improper use of your report       0.43      0.14      0.21        21\n",
      "                                            Incorrect information on your report       0.34      0.37      0.35       156\n",
      "                                        Loan modification,collection,foreclosure       0.52      0.55      0.53      1084\n",
      "                                        Loan servicing, payments, escrow account       0.52      0.36      0.43      1493\n",
      "Problem with a credit reporting company's investigation into an existing problem       0.79      0.19      0.31        57\n",
      "                                   Problem with fraud alerts or security freezes       1.00      0.00      0.00         1\n",
      "                                                    Settlement process and costs       0.37      0.06      0.10       235\n",
      "                                                      Struggling to pay mortgage       0.59      0.66      0.62      1674\n",
      "                                                  Trouble during payment process       0.67      0.80      0.73      3395\n",
      "                                Unable to get your credit report or credit score       1.00      0.00      0.00         5\n",
      "\n",
      "                                                                        accuracy                           0.58     10427\n",
      "                                                                       macro avg       0.60      0.31      0.33     10427\n",
      "                                                                    weighted avg       0.57      0.58      0.56     10427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test data.\n",
    "predictions = clf.predict(test_tfidf_vectors)\n",
    "\n",
    "# Create and print the classification report\n",
    "report = classification_report(test_labels, predictions, zero_division=1)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new question\n",
    "new_question = \"I need the credit score, but the website doesnt load\"\n",
    "\n",
    "# Transform the new question using the TF-IDF vectorizer\n",
    "new_question_vector = vectorizer.transform([new_question])\n",
    "\n",
    "# Use the classifier to predict the label for the new question\n",
    "predicted_label = clf.predict(new_question_vector)\n",
    "\n",
    "# Print the predicted label\n",
    "print(predicted_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
