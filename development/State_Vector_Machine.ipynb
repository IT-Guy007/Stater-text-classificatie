{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jeroendenotter/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing the libraries\n",
    "import pandas as pd\n",
    "\n",
    "# SSL error on MacOS\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Exporting model\n",
    "import joblib\n",
    "\n",
    "# Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word normalizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import edit_distance\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download(\"words\")\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Cleaning unneeded repetitive words\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql_query(\"SELECT * FROM 'mortgage complaints'\", \"sqlite:///StaterData.db\", parse_dates={'Date received': '%Y-%m-%d %H:%M:%S', 'Date sent to company': '%Y-%m-%d %H:%M:%S'})\n",
    "\n",
    "# Cleaning\n",
    "data[['Timely response?','Consumer disputed?']] = data[['Consumer disputed?','Timely response?']].replace({'Yes': True, 'No':False}).astype(bool)\n",
    "data['Consumer consent provided?'] = data['Consumer consent provided?'].replace({'Consent provided': True, '':False}).astype(bool)\n",
    "data[data['Consumer complaint narrative'].notna()]\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].str.lower()\n",
    "\n",
    "data = data.drop(columns=['Sub-issue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(n=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "remove_stopwords = lambda text: \" \".join([token for token in nltk.word_tokenize(text) if token.lower() not in stop_words])\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove names\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "def remove_names(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            text = text.replace(ent.text, '')\n",
    "    return text\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(remove_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise free text\n",
    "# Removing hashtags, urls, commas, etc.\n",
    "def clean_text(text):\n",
    "    # Remove numerical values\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation marks\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove links and URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove leading/trailing white space and convert to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing words, (play, playing, played) -> play, including spelling errors\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = SpellChecker()\n",
    "\n",
    "def lemmatize_word(word, tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        # Adjective\n",
    "        wn_tag = 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        # Verb\n",
    "        wn_tag = 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        # Noun\n",
    "        wn_tag = 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        # Adverb\n",
    "        wn_tag = 'r'\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    \n",
    "    if wn_tag:\n",
    "        lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "    else:\n",
    "        lemma = word\n",
    "    \n",
    "    return lemma\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Part-of-speech (POS) tag each word\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Iterate over each word and perform spell correction\n",
    "    corrected_tokens = [spell.correction(word) for word, _ in pos_tags]\n",
    "    \n",
    "    # Lemmatize each corrected word based on its POS tag\n",
    "    lemmas = [lemmatize_word(word, tag) for word, tag in zip(corrected_tokens, pos_tags)]\n",
    "    \n",
    "    # Join the lemmas back into a sentence\n",
    "    lemmatized_sentence = ' '.join(lemmas)\n",
    "    \n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(normalize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, creating into sets of words\n",
    "data['Consumer complaint narrative']= [word_tokenize(entry) for entry in data['Consumer complaint narrative']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove only tokenized words that are not alphabetic or only x\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(lambda x: [re.sub('[^a-zA-Z]+', '', word) for word in x])\n",
    "data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(lambda x: [word for word in x if not re.match('^x+$', word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets.\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data['Consumer complaint narrative'], data['Issue'], test_size=0.1)\n",
    "\n",
    "train_data = [' '.join(tokens) for tokens in train_data]\n",
    "test_data = [' '.join(tokens) for tokens in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer.\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r'\\b[a-zA-Z]+\\b',\n",
    "    analyzer=\"word\",\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    norm=None,\n",
    "    tokenizer=None,\n",
    "    preprocessor=None\n",
    ")\n",
    "# Fit the vectorizer to the training data.\n",
    "vectorizer.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer object with unigrams, bigrams, and trigrams as features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# Create a support vector machine classifier.\n",
    "clf = SVC(C=10, class_weight='balanced', gamma='scale', kernel='linear')\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "train_tfidf_vectors = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# Transform the testing data using the fitted vectorizer\n",
    "test_tfidf_vectors = vectorizer.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a support vector machine classifier.\n",
    "# clf = SVC(kernel='linear')\n",
    "\n",
    "# # Define the parameters to be tuned.\n",
    "# parameters = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'gamma': ['scale', 'auto'],\n",
    "#     'class_weight': ['balanced', None],\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV object to search over the parameter grid.\n",
    "# grid_search = GridSearchCV(clf, parameters, cv=5)\n",
    "\n",
    "# # Fit the GridSearchCV object to the training data.\n",
    "# grid_search.fit(train_tfidf_vectors, train_labels)\n",
    "\n",
    "# # Print the best hyperparameters and the corresponding mean cross-validated score.\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier on the TF-IDF vectors.\n",
    "clf.fit(train_tfidf_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the testing data\n",
    "pred_labels = clf.predict(test_tfidf_vectors)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(test_labels, pred_labels, normalize=True)\n",
    "count = data.shape[0] // 1000\n",
    "\n",
    "# Export the model to a file.\n",
    "filename = f\"model({count}K, {accuracy:.1%}).joblib\"\n",
    "joblib.dump(clf, filename)\n",
    "\n",
    "print(f\"Exported to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test data.\n",
    "predictions = clf.predict(test_tfidf_vectors)\n",
    "\n",
    "# Create and print the classification report\n",
    "report = classification_report(test_labels, predictions, zero_division=1)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new question\n",
    "new_question = \"I need the credit score, but the website doesnt load\"\n",
    "\n",
    "# Transform the new question using the TF-IDF vectorizer\n",
    "new_question_vector = vectorizer.transform([new_question])\n",
    "\n",
    "# Use the classifier to predict the label for the new question\n",
    "predicted_label = clf.predict(new_question_vector)\n",
    "\n",
    "# Print the predicted label\n",
    "print(predicted_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
