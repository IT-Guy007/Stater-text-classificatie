{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook TF-IDF model één team.\n",
    "\n",
    "## What is TF-IDF?\n",
    "\n",
    "The scores generated by the TF-IDF vectorizer are numerical values that indicate the importance of each word for each complaint compared to other complaints in the dataset.\n",
    "\n",
    "TF (Term Frequency) represents the frequency of a word in a specific complaint. A higher frequency means the word is more important for that particular complaint.\n",
    "IDF (Inverse Document Frequency) calculates the extent to which a word appears in the entire corpus of complaints. A higher IDF value indicates that the word is less important across all complaints in the dataset.\n",
    "\n",
    "The TF-IDF score is the product of the TF and IDF values. Words that appear frequently in a specific complaint but rarely in other complaints have a higher TF-IDF score and are considered more important for that particular complaint. Words that appear frequently in all complaints have a lower TF-IDF score and are considered less important.\n",
    "\n",
    "\n",
    "## In the code below, all needed imports for the following code are made up here.\n",
    "    - Pandas (Used for making dataframes)\n",
    "    - Seaborn (Used for visualizing the data in a graph)\n",
    "    - Matplotlib.pyplot (Used for generating an graph)\n",
    "    - Numpy (Used for generating arrays)\n",
    "    - Sklearn.model_selection (Used for splitting data in train and test data)\n",
    "    - Sklearn.feature_extraction.text (Used for giving an score to every word in a sentence)\n",
    "    - Nltk.corpus (Used for checking if word in english language)\n",
    "    - Sklearn.svm (Used for using the LinearSVC model)\n",
    "    - Nltk (Used for checking if word in english language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\SKIKK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, the option to display graphs inline will be set to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining the new column types for the dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define the data types for each column\n",
    "dtypes = {\n",
    "    'Date received': str,\n",
    "    'Product': \"category\",\n",
    "    'Sub-product': \"category\",\n",
    "    'Issue': \"category\",\n",
    "    'Sub-issue': \"category\",\n",
    "    'Consumer complaint narrative': str,\n",
    "    'Company public response': str,\n",
    "    'Company': \"category\",\n",
    "    'State': \"category\",\n",
    "    'ZIP code': str,\n",
    "    'Tags': \"category\",\n",
    "    'Consumer consent provided?': str,\n",
    "    'Submitted via': \"category\",\n",
    "    'Date sent to company': str,\n",
    "    'Company response to consumer': str,\n",
    "    'Timely response?': str,\n",
    "    'Consumer disputed?': str,\n",
    "    'Complaint ID': int\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data and assign it to the correct variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SKIKK\\AppData\\Local\\Temp\\ipykernel_7084\\206995529.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  DS1_data = pd.read_csv(\"../Data/complaints-2023-04-25_05_07.csv\", low_memory=False, dtype=dtypes, parse_dates=parse_dates, nrows=2000)\n",
      "C:\\Users\\SKIKK\\AppData\\Local\\Temp\\ipykernel_7084\\206995529.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  DS1_data = pd.read_csv(\"../Data/complaints-2023-04-25_05_07.csv\", low_memory=False, dtype=dtypes, parse_dates=parse_dates, nrows=2000)\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to parse as dates\n",
    "parse_dates = ['Product', 'Date received', 'Date sent to company']\n",
    "\n",
    "# Read the CSV file with specified data types and parse dates\n",
    "DS1_data = pd.read_csv(\"../Data/complaints-2023-04-25_05_07.csv\", low_memory=False, dtype=dtypes, parse_dates=parse_dates, nrows=2000)\n",
    "\n",
    "# Convert 'Timely response?' and 'Consumer disputed?' columns to boolean values\n",
    "DS1_data[['Timely response?', 'Consumer disputed?']] = DS1_data[['Timely response?', 'Consumer disputed?']].replace({'Yes': True, 'No': False}).astype(bool)\n",
    "\n",
    "# Convert 'Consumer consent provided?' column to boolean values\n",
    "DS1_data['Consumer consent provided?'] = DS1_data['Consumer consent provided?'].replace({'Consent provided': True, '': False}).astype(bool)\n",
    "\n",
    "# Drop rows with missing complaint narratives\n",
    "DS1_data.dropna(subset=['Consumer complaint narrative'], inplace=True)\n",
    "\n",
    "# Drop the 'Sub-issue' column as it is not needed\n",
    "DS1_data.drop(columns=['Sub-issue'], inplace=True)\n",
    "\n",
    "# Replace alle X occurences with emty strings, to avoid it from being the most important word.\n",
    "DS1_data['Consumer complaint narrative'] = DS1_data['Consumer complaint narrative'].str.replace('X', '')\n",
    "\n",
    "# Calculate the normalized count of issue categories\n",
    "IssueCountNormalized = DS1_data['Issue'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure accurate and meaningful tf-idf scores, it is recommended to enable smoothing (smooth_idf=True)\n",
    "and normalization (norm='l2') while using TfidfVectorizer.\n",
    "These settings help account for variations in text length and improve the quality of tf-idf scores.\n",
    "Since these settings are recommended by Python, we have enabled them here.\n",
    "\n",
    "Then we fit the already existing complaints and save their scores in the 'TF-IDF scores' row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer with optimized settings\n",
    "vectorizer = TfidfVectorizer(stop_words='english',              # Exclude common English words\n",
    "                             token_pattern=r'\\b[a-zA-Z]+\\b',    # Consider only alphabetic tokens\n",
    "                             analyzer='word',                   # Analyze at the word level\n",
    "                             use_idf=True,                      # Apply inverse document frequency weighting\n",
    "                             smooth_idf=True,                   # Apply smoothing to idf weights\n",
    "                             strip_accents='ascii',\n",
    "                             min_df=2,\n",
    "                             norm='l2')\n",
    "\n",
    "# Fit and transform the vectorizer to get an score per word in an array returned\n",
    "Vectorized_Data = vectorizer.fit_transform(DS1_data['Consumer complaint narrative'])\n",
    "\n",
    "# Add the scores array back into the dataframe.\n",
    "DS1_data['TF-IDF scores'] = list(Vectorized_Data.toarray())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "  Date received   Product                         Sub-product   \n0    2015-07-23  Mortgage                        FHA mortgage  \\\n1    2021-12-09  Mortgage                        FHA mortgage   \n2    2016-09-29  Mortgage                      Other mortgage   \n4    2021-12-09  Mortgage          Conventional home mortgage   \n5    2016-01-05  Mortgage  Home equity loan or line of credit   \n\n                                      Issue   \n0  Loan modification,collection,foreclosure  \\\n1                     Closing on a mortgage   \n2  Loan modification,collection,foreclosure   \n4                Struggling to pay mortgage   \n5  Loan servicing, payments, escrow account   \n\n                        Consumer complaint narrative   \n0  \" The California Homeowner Bill of Rights beca...  \\\n1  Hello, I am trying to refinance my property an...   \n2  Bank of America has caused to be recorded NOD ...   \n4  We have 2 loans w/ Loan Car . We were in a For...   \n5  My husband has died and I am filing bankruptcy...   \n\n                             Company public response   \n0   Company chooses not to provide a public response  \\\n1  Company has responded to the consumer and the ...   \n2  Company has responded to the consumer and the ...   \n4  Company has responded to the consumer and the ...   \n5                                                NaN   \n\n                                 Company State ZIP code            Tags   \n0                         CITIBANK, N.A.    CA    95965             NaN  \\\n1                          LoanCare, LLC    VA    20136             NaN   \n2  BANK OF AMERICA, NATIONAL ASSOCIATION    CA    92114             NaN   \n4                          LoanCare, LLC    IL    60471   Servicemember   \n5                         CITIBANK, N.A.    VT    05664  Older American   \n\n   Consumer consent provided? Submitted via Date sent to company   \n0                        True           Web           2015-07-30  \\\n1                        True           Web           2021-12-09   \n2                        True           Web           2016-09-29   \n4                        True           Web           2021-12-09   \n5                        True           Web           2016-01-05   \n\n  Company response to consumer  Timely response?  Consumer disputed?   \n0      Closed with explanation              True                True  \\\n1      Closed with explanation              True                True   \n2      Closed with explanation              True               False   \n4      Closed with explanation              True                True   \n5      Closed with explanation              True               False   \n\n   Complaint ID                                      TF-IDF scores   \n0       1484039  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \\\n1       4991921  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n2       2138230  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n4       4993695  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n5       1729048  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n\n                           top_word  \n0  [fair, underwriter, transparent]  \n1      [year, complicated, killing]  \n2           [america, nod, regular]  \n4        [payoff, properly, forced]  \n5        [account, lawyer, finance]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date received</th>\n      <th>Product</th>\n      <th>Sub-product</th>\n      <th>Issue</th>\n      <th>Consumer complaint narrative</th>\n      <th>Company public response</th>\n      <th>Company</th>\n      <th>State</th>\n      <th>ZIP code</th>\n      <th>Tags</th>\n      <th>Consumer consent provided?</th>\n      <th>Submitted via</th>\n      <th>Date sent to company</th>\n      <th>Company response to consumer</th>\n      <th>Timely response?</th>\n      <th>Consumer disputed?</th>\n      <th>Complaint ID</th>\n      <th>TF-IDF scores</th>\n      <th>top_word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-07-23</td>\n      <td>Mortgage</td>\n      <td>FHA mortgage</td>\n      <td>Loan modification,collection,foreclosure</td>\n      <td>\" The California Homeowner Bill of Rights beca...</td>\n      <td>Company chooses not to provide a public response</td>\n      <td>CITIBANK, N.A.</td>\n      <td>CA</td>\n      <td>95965</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>Web</td>\n      <td>2015-07-30</td>\n      <td>Closed with explanation</td>\n      <td>True</td>\n      <td>True</td>\n      <td>1484039</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[fair, underwriter, transparent]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-12-09</td>\n      <td>Mortgage</td>\n      <td>FHA mortgage</td>\n      <td>Closing on a mortgage</td>\n      <td>Hello, I am trying to refinance my property an...</td>\n      <td>Company has responded to the consumer and the ...</td>\n      <td>LoanCare, LLC</td>\n      <td>VA</td>\n      <td>20136</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>Web</td>\n      <td>2021-12-09</td>\n      <td>Closed with explanation</td>\n      <td>True</td>\n      <td>True</td>\n      <td>4991921</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[year, complicated, killing]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-09-29</td>\n      <td>Mortgage</td>\n      <td>Other mortgage</td>\n      <td>Loan modification,collection,foreclosure</td>\n      <td>Bank of America has caused to be recorded NOD ...</td>\n      <td>Company has responded to the consumer and the ...</td>\n      <td>BANK OF AMERICA, NATIONAL ASSOCIATION</td>\n      <td>CA</td>\n      <td>92114</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>Web</td>\n      <td>2016-09-29</td>\n      <td>Closed with explanation</td>\n      <td>True</td>\n      <td>False</td>\n      <td>2138230</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[america, nod, regular]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-12-09</td>\n      <td>Mortgage</td>\n      <td>Conventional home mortgage</td>\n      <td>Struggling to pay mortgage</td>\n      <td>We have 2 loans w/ Loan Car . We were in a For...</td>\n      <td>Company has responded to the consumer and the ...</td>\n      <td>LoanCare, LLC</td>\n      <td>IL</td>\n      <td>60471</td>\n      <td>Servicemember</td>\n      <td>True</td>\n      <td>Web</td>\n      <td>2021-12-09</td>\n      <td>Closed with explanation</td>\n      <td>True</td>\n      <td>True</td>\n      <td>4993695</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[payoff, properly, forced]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2016-01-05</td>\n      <td>Mortgage</td>\n      <td>Home equity loan or line of credit</td>\n      <td>Loan servicing, payments, escrow account</td>\n      <td>My husband has died and I am filing bankruptcy...</td>\n      <td>NaN</td>\n      <td>CITIBANK, N.A.</td>\n      <td>VT</td>\n      <td>05664</td>\n      <td>Older American</td>\n      <td>True</td>\n      <td>Web</td>\n      <td>2016-01-05</td>\n      <td>Closed with explanation</td>\n      <td>True</td>\n      <td>False</td>\n      <td>1729048</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[account, lawyer, finance]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines the dictionary of english words\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\n",
    "# Gets the list of words from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get unique values from the \"Issue\" column\n",
    "unique_issues = DS1_data['Issue'].unique()\n",
    "\n",
    "# Concatenate the unique issues into a single string\n",
    "all_issues = ' '.join(unique_issues)\n",
    "\n",
    "# Split the concatenated string into individual words\n",
    "all_words = all_issues.split()\n",
    "\n",
    "# Get unique words\n",
    "Mortgage_Terms = set(all_words)\n",
    "\n",
    "# Set an empty top 3 words list.\n",
    "top_words = []\n",
    "\n",
    "\n",
    "for i in range(Vectorized_Data.shape[0]):\n",
    "    # Get the array with scores from this row\n",
    "    row_scores = Vectorized_Data[i].toarray()[0]\n",
    "\n",
    "    # Generate a dictionary with each word and then the Scores\n",
    "    scores_dict = {name: score for name, score in zip(feature_names,row_scores)}\n",
    "\n",
    "    # Loop over all the words and adjust the score\n",
    "    for name in feature_names:\n",
    "        # If term is in categoryname, add to score\n",
    "        if name in Mortgage_Terms:\n",
    "            scores_dict[name] *= (1 + 0.02*len(name) + 0.2)\n",
    "        else:\n",
    "            scores_dict[name] *= (1 + 0.02*len(name))\n",
    "\n",
    "    # Create a new array of adjusted scores in the same order as the feature names\n",
    "    adjusted_scores = np.zeros(len(feature_names))\n",
    "    for i, term in enumerate(feature_names):\n",
    "        adjusted_scores[i] = scores_dict[term]\n",
    "\n",
    "    #Set the top 3 to a empty list.\n",
    "    Top3Words = []\n",
    "\n",
    "    # Get the index of the highest scoring word\n",
    "    max_index = adjusted_scores.argmax()\n",
    "\n",
    "    # Iterate until 3 top words found or list of words empty\n",
    "    while max_index != 0 and len(Top3Words) <= 2:\n",
    "        #Check if there are any vowels in the topword, if not select new word\n",
    "        while True:\n",
    "            vowels = {\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"}\n",
    "            # Get the corresponding word\n",
    "            top_word = feature_names[max_index]\n",
    "            if any(char in vowels for char in feature_names[max_index]) and top_word in english_vocab:\n",
    "                Top3Words.append(top_word)\n",
    "            break\n",
    "\n",
    "        #Sets the current score of this word to 0 to select the second most popular word\n",
    "        adjusted_scores[max_index] = 0\n",
    "        max_index = adjusted_scores.argmax()\n",
    "    # Voeg het bijbehorende woord toe aan de lijst van top_words\n",
    "    top_words.append(Top3Words)\n",
    "\n",
    "# Add the list of topwords to the dataframe\n",
    "DS1_data['top_word'] = top_words\n",
    "\n",
    "Traindata,Testdata = train_test_split(DS1_data,test_size=0.25,random_state = 0)\n",
    "\n",
    "# Save the generated table to a new csv Dataset\n",
    "DS1_data.head()\n",
    "#DS1_data.to_csv('TrainData.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following cells, we will perform tf-idf conversion on the string data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function creates an array of every word in text and gives it an score\n",
    "def tfidf_custom_scoring(input_text):\n",
    "    # Fit the data to the vectorizer\n",
    "    vectorizer.fit(DS1_data['Consumer complaint narrative'])\n",
    "\n",
    "    # Adds the input from the user to the fit using an transform\n",
    "    transformed_data = vectorizer.transform([input_text])\n",
    "\n",
    "    # Get all the feature name(words instead of numbers) corresponding to the array\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # calculate initial scores and store them in a dictionary\n",
    "    scores_dict = {name: score for name, score in zip(feature_names, transformed_data.toarray()[0])}\n",
    "\n",
    "    # adjust scores based on term length and update the dictionary\n",
    "    for name in feature_names:\n",
    "        if name in Mortgage_Terms:\n",
    "            scores_dict[name] *= (1 + 0.01*len(name) + 1.5)\n",
    "        else:\n",
    "            scores_dict[name] *= (1 + 0.01*len(name))\n",
    "\n",
    "    # create a new array of adjusted scores in the same order as the feature names\n",
    "    adjusted_scores = np.zeros(len(feature_names))\n",
    "    for INDEX, term in enumerate(feature_names):\n",
    "        adjusted_scores[INDEX] = scores_dict[term]\n",
    "\n",
    "    # return a tuple of the feature names and adjusted scores\n",
    "    return feature_names, adjusted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_input = input(\"What question do you want to categorize? \")\n",
    "def Classifiy_string(user_input):\n",
    "    # Get an array of words with corresponding scores\n",
    "    data = tfidf_custom_scoring(user_input)\n",
    "\n",
    "    # Create a dictionary with feature names as keys and scores as values\n",
    "    scores_dict = {name: score for name, score in zip(data[0], data[1])}\n",
    "\n",
    "    # Create a new array of adjusted scores in the same order as the feature names\n",
    "    adjusted_scores = np.zeros(len(feature_names))\n",
    "    for i, term in enumerate(feature_names):\n",
    "        adjusted_scores[i] = scores_dict[term]\n",
    "\n",
    "    # Set the top 3 words to a list\n",
    "    Top3Words = []\n",
    "\n",
    "    def get_top3_words():\n",
    "        # Get the index of the highest score in the scores array\n",
    "        index_max = adjusted_scores.argmax()\n",
    "\n",
    "        while index_max != 0 and len(Top3Words) <= 2:\n",
    "            # Check if there are any vowels in the top word, if not, select a new word\n",
    "            while True:\n",
    "                vowels = {\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"}\n",
    "                # Get the corresponding word\n",
    "                top_word = feature_names[index_max]\n",
    "                if any(char in vowels for char in top_word) and top_word in english_vocab:\n",
    "                    Top3Words.append(top_word)\n",
    "                break\n",
    "\n",
    "            # Get the corresponding word\n",
    "            top_word = str(data[0][index_max])\n",
    "\n",
    "            # If the word is in the English vocabulary, add it to the top 3 list\n",
    "            if top_word in english_vocab:\n",
    "                Top3Words.append(top_word)\n",
    "\n",
    "            # Set the current score of this word to 0 to select the second most popular word\n",
    "            adjusted_scores[index_max] = 0\n",
    "            index_max = adjusted_scores.argmax()\n",
    "\n",
    "    get_top3_words()\n",
    "\n",
    "    # Check all past result classifications\n",
    "    def check_corresponding_word(relevant_word):\n",
    "        return DS1_data[(DS1_data[\"top_word\"].str[0] == relevant_word[0]) & DS1_data[\"top_word\"].str[1].isin(relevant_word)]\n",
    "\n",
    "    filtered_df = check_corresponding_word(Top3Words)\n",
    "\n",
    "    # Count the occurrences of each issue and get the most common one, as long as there are words left\n",
    "    value_counts = filtered_df[\"Issue\"].value_counts()\n",
    "    NormalizedTable = pd.concat([IssueCountNormalized, value_counts], axis=1, keys=('perc', 'valuecount'))\n",
    "    Endscores = NormalizedTable.valuecount / NormalizedTable.perc\n",
    "    NormalizedTable[\"Endscores\"] = Endscores\n",
    "    NormalizedTable[\"IssueName\"] = IssueCountNormalized.keys().tolist()\n",
    "\n",
    "    Toprow = NormalizedTable.loc[NormalizedTable['Endscores'].idxmax()]\n",
    "    return Toprow.IssueName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the algorithm we made above, we test the classification on 100 rows and give back an percentage of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct voorspeld: 69%\n",
      "Fout voorspeld: 31%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Varaible storing if categorising was correct\n",
    "CorrectPrognosed = 0\n",
    "WrongPrognosed = 0\n",
    "\n",
    "for index, row in Testdata.head(100).iterrows():\n",
    "    QuestionCateogryPrediction = Classifiy_string(row[\"Consumer complaint narrative\"])\n",
    "    if(QuestionCateogryPrediction == DS1_data.at[index,\"Issue\"]):\n",
    "        CorrectPrognosed += 1\n",
    "    else:\n",
    "        WrongPrognosed += 1\n",
    "\n",
    "print(\"Correct voorspeld: \" + str(CorrectPrognosed) + \"%\")\n",
    "print(\"Fout voorspeld: \" + str(WrongPrognosed) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your question will be in the following category: Trouble during payment process\n"
     ]
    }
   ],
   "source": [
    "print(\"Your question will be in the following category: \"+Classifiy_string(input(\"What question do you want to categorize? \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In conclusion, the training model that has been developed to extract the top three most important words from a given question and predict the corresponding category has shown promising results. With an accuracy rate of 69%, the model has demonstrated its ability to correctly identify the appropriate category in a considerable number of cases.\n",
    "\n",
    "This model's performance is notable considering the complexity of the task it undertakes. By focusing on the most significant words, it efficiently captures the essence of the question and utilizes this information to make accurate predictions. While the 69% accuracy rate indicates room for improvement, it is a commendable achievement and serves as a strong foundation for future enhancements.\n",
    "\n",
    "The successful implementation of this training model opens up various possibilities for practical applications. It can be utilized in various domains where categorization of questions is crucial, such as customer support systems, information retrieval systems, and automated chatbots. The ability to swiftly categorize questions based on a few essential words can enhance user experiences and streamline processes, saving time and resources.\n",
    "\n",
    "Moving forward, further improvements can be made to enhance the model's accuracy. Exploring advanced natural language processing techniques, such as contextual embeddings or attention mechanisms, could help capture more nuanced relationships and context between words. Additionally, expanding the training dataset and fine-tuning the model parameters can contribute to better generalization and performance.\n",
    "\n",
    "In conclusion, the developed training model that extracts the top three most important words from a question and predicts the corresponding category has demonstrated promising accuracy, achieving a success rate of 69%. While there is scope for improvement, this model lays a solid foundation for future advancements in question categorization, facilitating more efficient and effective systems across various industries and domains."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
